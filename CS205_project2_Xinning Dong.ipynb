{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import time\n",
    "\n",
    "def forward_selection(data, num_features, num_instances):\n",
    "    best_so_far_accuracy = 0.0\n",
    "    #initialize an empty set\n",
    "    best_subset = [] #initialize the best subset so far for a designated number of features\n",
    "    best_total = [] #the best set among all of different numbers of features.\n",
    "    for i in range(num_features): #start from 0 feature\n",
    "        #flags for checking if the new feature is added to total or subset\n",
    "        add_total = -1\n",
    "        best_subset_accuracy = 0.0\n",
    "        print(\"On the \" , i , \"th level of the search tree\")\n",
    "\n",
    "        for j in range(1, num_features + 1):\n",
    "            if j not in best_subset:\n",
    "                cur_subset = copy.deepcopy(best_subset)\n",
    "                cur_subset.append(j)\n",
    "                \n",
    "                accuracy = leave_one_out_cross_validation(data, cur_subset, num_instances)\n",
    "                print(\"\\tUsing feature(s) \", cur_subset, \" accuracy is \", accuracy, \"%\")\n",
    "                if accuracy > best_so_far_accuracy:\n",
    "                    best_so_far_accuracy = accuracy\n",
    "                    add_total = j\n",
    "                if accuracy > best_subset_accuracy:\n",
    "                    best_subset_accuracy = accuracy\n",
    "                    add_subset = j\n",
    "        \n",
    "        if add_total != -1:\n",
    "            best_subset.append(add_total)\n",
    "            best_total.append(add_total)\n",
    "            print(\"\\n\\nFeature set \", best_subset, \" was best, accuracy is \", best_so_far_accuracy, \"%\\n\\n\")\n",
    "        else:\n",
    "            print(\"\\n\\n(Warning, Accuracy has decreases! Continuing search in case of local maxima)\")\n",
    "            best_subset.append(add_subset)\n",
    "            print(\"Feature set \", best_subset, \" was best, accuracy is \", best_subset_accuracy, \"%\\n\\n\")\n",
    "    #Finished looping all numbers of features\n",
    "    print(\"Finished search!! The best feature subset is \", best_total, \" which has an accuracy of \", best_so_far_accuracy, \"%\")\n",
    "    \n",
    "\n",
    "\n",
    "def backward_elimination(data, num_features, num_instances):\n",
    "    best_so_far_accuracy = 0.0\n",
    "    #initialize a full subset\n",
    "    best_subset = [i+1 for i in range(num_features)] #initialize the best subset so far for a designated number of features\n",
    "    best_total = [i+1 for i in range(num_features)] #the best set among all of different numbers of features.\n",
    "    for i in range(num_features):\n",
    "         #flags for checking if the new feature is added to total or subset\n",
    "        remove_total = -1\n",
    "        best_subset_accuracy = 0.0\n",
    "        print(\"On the \" , i , \"th level of the search tree\")\n",
    "        \n",
    "        for j in range(1, num_features + 1):\n",
    "            if j in best_subset:\n",
    "                cur_subset = copy.deepcopy(best_subset)\n",
    "                cur_subset.remove(j)\n",
    "                \n",
    "                accuracy = leave_one_out_cross_validation(data, cur_subset, num_instances)\n",
    "                print(\"\\tUsing feature(s) \", cur_subset, \" accuracy is \", accuracy, \"%\")\n",
    "                if accuracy > best_so_far_accuracy:\n",
    "                    best_so_far_accuracy = accuracy\n",
    "                    remove_total = j\n",
    "                if accuracy > best_subset_accuracy:\n",
    "                    best_subset_accuracy = accuracy\n",
    "                    remove_subset = j\n",
    "                    \n",
    "        if remove_total != -1:\n",
    "            best_subset.remove(remove_total)\n",
    "            best_total.remove(remove_total)\n",
    "            print(\"\\n\\nFeature set \", best_subset, \" was best, accuracy is \", best_so_far_accuracy, \"%\\n\\n\")\n",
    "        else:\n",
    "            print(\"\\n\\n(Warning, Accuracy has decreases! Continuing search in case of local maxima)\")\n",
    "            best_subset.remove(remove_subset)\n",
    "            print(\"Feature set \", best_subset, \" was best, accuracy is \", best_subset_accuracy, \"%\\n\\n\")\n",
    "    #Finished looping all numbers of features\n",
    "    print(\"Finished search!! The best feature subset is \", best_total, \" which has an accuracy of \", best_so_far_accuracy, \"%\")\n",
    "    \n",
    "\n",
    "    \n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_cross_validation(data, cur_subset, num_instances):\n",
    "    num_corr_classfied = 0\n",
    "    for i in range(num_instances):\n",
    "        nearest_neighbor_location = float(\"inf\")\n",
    "        nearest_neighbor_distance = float(\"inf\")\n",
    "        for j in range(num_instances):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                distance = 0\n",
    "                for k in range(len(cur_subset)):\n",
    "                    distance = distance + pow((data[j][cur_subset[k]] - data[i][cur_subset[k]]), 2)\n",
    "                distance = math.sqrt(distance)\n",
    "                \n",
    "                if distance < nearest_neighbor_distance:\n",
    "                    nearest_neighbor_distance = distance\n",
    "                    nearest_neighbor_location = j\n",
    "        \n",
    "        neighbor = nearest_neighbor_location\n",
    "        \n",
    "        if data[neighbor][0] == data[i][0]:\n",
    "            num_corr_classfied += 1\n",
    "    \n",
    "    accuracy = (num_corr_classfied/num_instances) *100\n",
    "    return accuracy\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    print(\"Welcome to Xinning Dong's Feature Selection Algorithm.\")\n",
    "    file = input(\"Type in the name of the file to test: \")\n",
    "    data = open(file, 'r')\n",
    "    \n",
    "    readline = data.readline()\n",
    "    num_features = len(readline.split()) - 1\n",
    "    \n",
    "    data.seek(0)\n",
    "    num_instances = sum(1 for i in data)\n",
    "    data.seek(0)\n",
    "    \n",
    "    #load data as a 2D array\n",
    "    instances = [[] for i in range(num_instances)]\n",
    "    for i in range(num_instances):\n",
    "        instances[i] = [float(j) for j in data.readline().split()]\n",
    "        \n",
    "    \n",
    "    print(\"Type the number of the algorithm you want to run.\")\n",
    "    print(\"1) Forward Selection\")\n",
    "    print(\"2) Backward Elimination\")\n",
    "    switch = int(input())\n",
    "    print(\"This data set has \", num_features, \" features (not including the class attribute), with \", num_instances,\n",
    "          \" instances.\" )\n",
    "        \n",
    "    \n",
    "    #normalize data by deducting average of the data and then devide by the standard deviation of the data\n",
    "    avg = []\n",
    "    for i in range(1, num_features + 1):\n",
    "        avg.append((sum(row[i] for row in instances)) / num_instances)\n",
    "    \n",
    "    std = []\n",
    "    for i in range(1, num_features + 1):\n",
    "        std.append(math.sqrt(sum(pow((row[i] - avg[i-1]), 2) for row in instances) / num_instances))\n",
    "    \n",
    "    for i in range(num_instances):\n",
    "        for j in range(1, num_features + 1):\n",
    "            instances[i][j] = (instances[i][j] - avg[j-1])/std[j-1]\n",
    "    \n",
    "    #run with all features\n",
    "    all_subset = []\n",
    "    for i in range(1, num_features + 1):\n",
    "        all_subset.append(i)\n",
    "    accuracy = leave_one_out_cross_validation(instances, all_subset, num_instances)\n",
    "    print(\"Running nearest neighbor with all \", num_features, \" features, using \\\"leaving-one-out\\\" evaluation, \n",
    "          I get an accuracy of \", accuracy, \"%\")    \n",
    "    \n",
    "    print(\"Beginning search.\\n\\n\")\n",
    "    \n",
    "    if switch == 1:\n",
    "        forward_selection(instances, num_features, num_instances)\n",
    "    elif switch == 2:\n",
    "        backward_elimination(instances, num_features, num_instances)\n",
    "    \n",
    "\n",
    "def timed_run():\n",
    "    start = time.time()\n",
    "    run()\n",
    "    end = time.time()\n",
    "    print(\"Function finishes in \", end-start, \"seconds.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Xinning Dong's Feature Selection Algorithm.\n",
      "Type in the name of the file to test: CS205_small_testdata__25.txt\n",
      "Type the number of the algorithm you want to run.\n",
      "1) Forward Selection\n",
      "2) Backward Elimination\n",
      "1\n",
      "This data set has  10  features (not including the class attribute), with  300  instances.\n",
      "Running nearest neighbor with all  10  features, using \"leaving-one-out\" evaluation, I get an accuracy of  76.66666666666667 %\n",
      "Beginning search.\n",
      "\n",
      "\n",
      "On the  0 th level of the search tree\n",
      "\tUsing feature(s)  [1]  accuracy is  70.33333333333334 %\n",
      "\tUsing feature(s)  [2]  accuracy is  72.66666666666667 %\n",
      "\tUsing feature(s)  [3]  accuracy is  68.33333333333333 %\n",
      "\tUsing feature(s)  [4]  accuracy is  67.66666666666666 %\n",
      "\tUsing feature(s)  [5]  accuracy is  68.66666666666667 %\n",
      "\tUsing feature(s)  [6]  accuracy is  86.33333333333333 %\n",
      "\tUsing feature(s)  [7]  accuracy is  71.66666666666667 %\n",
      "\tUsing feature(s)  [8]  accuracy is  66.66666666666666 %\n",
      "\tUsing feature(s)  [9]  accuracy is  67.33333333333333 %\n",
      "\tUsing feature(s)  [10]  accuracy is  65.0 %\n",
      "\n",
      "\n",
      "Feature set  [6]  was best, accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "On the  1 th level of the search tree\n",
      "\tUsing feature(s)  [6, 1]  accuracy is  88.0 %\n",
      "\tUsing feature(s)  [6, 2]  accuracy is  85.33333333333334 %\n",
      "\tUsing feature(s)  [6, 3]  accuracy is  84.33333333333334 %\n",
      "\tUsing feature(s)  [6, 4]  accuracy is  86.33333333333333 %\n",
      "\tUsing feature(s)  [6, 5]  accuracy is  85.0 %\n",
      "\tUsing feature(s)  [6, 7]  accuracy is  94.0 %\n",
      "\tUsing feature(s)  [6, 8]  accuracy is  87.66666666666667 %\n",
      "\tUsing feature(s)  [6, 9]  accuracy is  86.66666666666667 %\n",
      "\tUsing feature(s)  [6, 10]  accuracy is  85.0 %\n",
      "\n",
      "\n",
      "Feature set  [6, 7]  was best, accuracy is  94.0 %\n",
      "\n",
      "\n",
      "On the  2 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 1]  accuracy is  90.0 %\n",
      "\tUsing feature(s)  [6, 7, 2]  accuracy is  91.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 3]  accuracy is  91.0 %\n",
      "\tUsing feature(s)  [6, 7, 4]  accuracy is  91.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 5]  accuracy is  91.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8]  accuracy is  93.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 9]  accuracy is  93.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 10]  accuracy is  93.66666666666667 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8]  was best, accuracy is  93.66666666666667 %\n",
      "\n",
      "\n",
      "On the  3 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 1]  accuracy is  86.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 2]  accuracy is  88.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 3]  accuracy is  88.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 4]  accuracy is  89.0 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5]  accuracy is  89.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 8, 9]  accuracy is  89.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 10]  accuracy is  88.0 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5]  was best, accuracy is  89.66666666666666 %\n",
      "\n",
      "\n",
      "On the  4 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 1]  accuracy is  84.33333333333334 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 2]  accuracy is  83.33333333333334 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3]  accuracy is  87.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 4]  accuracy is  87.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 9]  accuracy is  86.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 10]  accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3]  was best, accuracy is  87.33333333333333 %\n",
      "\n",
      "\n",
      "On the  5 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 1]  accuracy is  85.33333333333334 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 2]  accuracy is  81.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4]  accuracy is  86.0 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 9]  accuracy is  84.0 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 10]  accuracy is  83.33333333333334 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4]  was best, accuracy is  86.0 %\n",
      "\n",
      "\n",
      "On the  6 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 1]  accuracy is  78.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 2]  accuracy is  81.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 9]  accuracy is  80.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10]  accuracy is  82.66666666666667 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4, 10]  was best, accuracy is  82.66666666666667 %\n",
      "\n",
      "\n",
      "On the  7 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 1]  accuracy is  81.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 2]  accuracy is  81.0 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 9]  accuracy is  78.66666666666666 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4, 10, 1]  was best, accuracy is  81.33333333333333 %\n",
      "\n",
      "\n",
      "On the  8 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 1, 2]  accuracy is  79.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 1, 9]  accuracy is  78.33333333333333 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4, 10, 1, 2]  was best, accuracy is  79.66666666666666 %\n",
      "\n",
      "\n",
      "On the  9 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 1, 2, 9]  accuracy is  76.66666666666667 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4, 10, 1, 2, 9]  was best, accuracy is  76.66666666666667 %\n",
      "\n",
      "\n",
      "Finished search!! The best feature subset is  [6, 7]  which has an accuracy of  94.0 %\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Xinning Dong's Feature Selection Algorithm.\n",
      "Type in the name of the file to test: CS205_small_testdata__25.txt\n",
      "Type the number of the algorithm you want to run.\n",
      "1) Forward Selection\n",
      "2) Backward Elimination\n",
      "2\n",
      "This data set has  10  features (not including the class attribute), with  300  instances.\n",
      "Running nearest neighbor with all  10  features, using \"leaving-one-out\" evaluation, I get an accuracy of  76.66666666666667 %\n",
      "Beginning search.\n",
      "\n",
      "\n",
      "On the  0 th level of the search tree\n",
      "\tUsing feature(s)  [2, 3, 4, 5, 6, 7, 8, 9, 10]  accuracy is  77.66666666666666 %\n",
      "\tUsing feature(s)  [1, 3, 4, 5, 6, 7, 8, 9, 10]  accuracy is  78.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 4, 5, 6, 7, 8, 9, 10]  accuracy is  78.0 %\n",
      "\tUsing feature(s)  [1, 2, 3, 5, 6, 7, 8, 9, 10]  accuracy is  77.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 7, 8, 9, 10]  accuracy is  80.0 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 7, 8, 9, 10]  accuracy is  73.66666666666667 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 6, 8, 9, 10]  accuracy is  71.66666666666667 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 6, 7, 9, 10]  accuracy is  76.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 6, 7, 8, 10]  accuracy is  79.66666666666666 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 6, 7, 8, 9]  accuracy is  76.66666666666667 %\n",
      "\n",
      "\n",
      "Feature set  [1, 2, 3, 4, 6, 7, 8, 9, 10]  was best, accuracy is  80.0 %\n",
      "\n",
      "\n",
      "On the  1 th level of the search tree\n",
      "\tUsing feature(s)  [2, 3, 4, 6, 7, 8, 9, 10]  accuracy is  78.66666666666666 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 8, 9, 10]  accuracy is  82.0 %\n",
      "\tUsing feature(s)  [1, 2, 4, 6, 7, 8, 9, 10]  accuracy is  77.66666666666666 %\n",
      "\tUsing feature(s)  [1, 2, 3, 6, 7, 8, 9, 10]  accuracy is  78.0 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 7, 8, 9, 10]  accuracy is  74.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 8, 9, 10]  accuracy is  75.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 7, 9, 10]  accuracy is  77.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 7, 8, 10]  accuracy is  81.0 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 7, 8, 9]  accuracy is  78.0 %\n",
      "\n",
      "\n",
      "Feature set  [1, 3, 4, 6, 7, 8, 9, 10]  was best, accuracy is  82.0 %\n",
      "\n",
      "\n",
      "On the  2 th level of the search tree\n",
      "\tUsing feature(s)  [3, 4, 6, 7, 8, 9, 10]  accuracy is  82.66666666666667 %\n",
      "\tUsing feature(s)  [1, 4, 6, 7, 8, 9, 10]  accuracy is  80.66666666666666 %\n",
      "\tUsing feature(s)  [1, 3, 6, 7, 8, 9, 10]  accuracy is  83.0 %\n",
      "\tUsing feature(s)  [1, 3, 4, 7, 8, 9, 10]  accuracy is  74.33333333333333 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 8, 9, 10]  accuracy is  76.66666666666667 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 9, 10]  accuracy is  82.33333333333334 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 8, 10]  accuracy is  84.0 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 8, 9]  accuracy is  80.0 %\n",
      "\n",
      "\n",
      "Feature set  [1, 3, 4, 6, 7, 8, 10]  was best, accuracy is  84.0 %\n",
      "\n",
      "\n",
      "On the  3 th level of the search tree\n",
      "\tUsing feature(s)  [3, 4, 6, 7, 8, 10]  accuracy is  85.33333333333334 %\n",
      "\tUsing feature(s)  [1, 4, 6, 7, 8, 10]  accuracy is  86.33333333333333 %\n",
      "\tUsing feature(s)  [1, 3, 6, 7, 8, 10]  accuracy is  82.66666666666667 %\n",
      "\tUsing feature(s)  [1, 3, 4, 7, 8, 10]  accuracy is  74.0 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 8, 10]  accuracy is  78.33333333333333 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 10]  accuracy is  85.66666666666667 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 8]  accuracy is  81.0 %\n",
      "\n",
      "\n",
      "Feature set  [1, 4, 6, 7, 8, 10]  was best, accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "On the  4 th level of the search tree\n",
      "\tUsing feature(s)  [4, 6, 7, 8, 10]  accuracy is  87.0 %\n",
      "\tUsing feature(s)  [1, 6, 7, 8, 10]  accuracy is  86.0 %\n",
      "\tUsing feature(s)  [1, 4, 7, 8, 10]  accuracy is  71.66666666666667 %\n",
      "\tUsing feature(s)  [1, 4, 6, 8, 10]  accuracy is  77.33333333333333 %\n",
      "\tUsing feature(s)  [1, 4, 6, 7, 10]  accuracy is  85.66666666666667 %\n",
      "\tUsing feature(s)  [1, 4, 6, 7, 8]  accuracy is  84.66666666666667 %\n",
      "\n",
      "\n",
      "Feature set  [4, 6, 7, 8, 10]  was best, accuracy is  87.0 %\n",
      "\n",
      "\n",
      "On the  5 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 10]  accuracy is  88.0 %\n",
      "\tUsing feature(s)  [4, 7, 8, 10]  accuracy is  72.66666666666667 %\n",
      "\tUsing feature(s)  [4, 6, 8, 10]  accuracy is  81.66666666666667 %\n",
      "\tUsing feature(s)  [4, 6, 7, 10]  accuracy is  89.0 %\n",
      "\tUsing feature(s)  [4, 6, 7, 8]  accuracy is  89.0 %\n",
      "\n",
      "\n",
      "Feature set  [4, 6, 7, 10]  was best, accuracy is  89.0 %\n",
      "\n",
      "\n",
      "On the  6 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 10]  accuracy is  93.66666666666667 %\n",
      "\tUsing feature(s)  [4, 7, 10]  accuracy is  71.33333333333334 %\n",
      "\tUsing feature(s)  [4, 6, 10]  accuracy is  82.33333333333334 %\n",
      "\tUsing feature(s)  [4, 6, 7]  accuracy is  91.66666666666666 %\n",
      "\n",
      "\n",
      "Feature set  [6, 7, 10]  was best, accuracy is  93.66666666666667 %\n",
      "\n",
      "\n",
      "On the  7 th level of the search tree\n",
      "\tUsing feature(s)  [7, 10]  accuracy is  73.0 %\n",
      "\tUsing feature(s)  [6, 10]  accuracy is  85.0 %\n",
      "\tUsing feature(s)  [6, 7]  accuracy is  94.0 %\n",
      "\n",
      "\n",
      "Feature set  [6, 7]  was best, accuracy is  94.0 %\n",
      "\n",
      "\n",
      "On the  8 th level of the search tree\n",
      "\tUsing feature(s)  [7]  accuracy is  71.66666666666667 %\n",
      "\tUsing feature(s)  [6]  accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6]  was best, accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "On the  9 th level of the search tree\n",
      "\tUsing feature(s)  []  accuracy is  81.66666666666667 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  []  was best, accuracy is  81.66666666666667 %\n",
      "\n",
      "\n",
      "Finished search!! The best feature subset is  [6, 7]  which has an accuracy of  94.0 %\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Xinning Dong's Feature Selection Algorithm.\n",
      "Type in the name of the file to test: CS205_small_testdata__25.txt\n",
      "Type the number of the algorithm you want to run.\n",
      "1) Forward Selection\n",
      "2) Backward Elimination\n",
      "1\n",
      "This data set has  10  features (not including the class attribute), with  300  instances.\n",
      "Running nearest neighbor with all  10  features, using \"leaving-one-out\" evaluation, I get an accuracy of  76.66666666666667 %\n",
      "Beginning search.\n",
      "\n",
      "\n",
      "On the  0 th level of the search tree\n",
      "\tUsing feature(s)  [1]  accuracy is  70.33333333333334 %\n",
      "\tUsing feature(s)  [2]  accuracy is  72.66666666666667 %\n",
      "\tUsing feature(s)  [3]  accuracy is  68.33333333333333 %\n",
      "\tUsing feature(s)  [4]  accuracy is  67.66666666666666 %\n",
      "\tUsing feature(s)  [5]  accuracy is  68.66666666666667 %\n",
      "\tUsing feature(s)  [6]  accuracy is  86.33333333333333 %\n",
      "\tUsing feature(s)  [7]  accuracy is  71.66666666666667 %\n",
      "\tUsing feature(s)  [8]  accuracy is  66.66666666666666 %\n",
      "\tUsing feature(s)  [9]  accuracy is  67.33333333333333 %\n",
      "\tUsing feature(s)  [10]  accuracy is  65.0 %\n",
      "\n",
      "\n",
      "Feature set  [6]  was best, accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "On the  1 th level of the search tree\n",
      "\tUsing feature(s)  [6, 1]  accuracy is  88.0 %\n",
      "\tUsing feature(s)  [6, 2]  accuracy is  85.33333333333334 %\n",
      "\tUsing feature(s)  [6, 3]  accuracy is  84.33333333333334 %\n",
      "\tUsing feature(s)  [6, 4]  accuracy is  86.33333333333333 %\n",
      "\tUsing feature(s)  [6, 5]  accuracy is  85.0 %\n",
      "\tUsing feature(s)  [6, 7]  accuracy is  94.0 %\n",
      "\tUsing feature(s)  [6, 8]  accuracy is  87.66666666666667 %\n",
      "\tUsing feature(s)  [6, 9]  accuracy is  86.66666666666667 %\n",
      "\tUsing feature(s)  [6, 10]  accuracy is  85.0 %\n",
      "\n",
      "\n",
      "Feature set  [6, 7]  was best, accuracy is  94.0 %\n",
      "\n",
      "\n",
      "On the  2 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 1]  accuracy is  90.0 %\n",
      "\tUsing feature(s)  [6, 7, 2]  accuracy is  91.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 3]  accuracy is  91.0 %\n",
      "\tUsing feature(s)  [6, 7, 4]  accuracy is  91.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 5]  accuracy is  91.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8]  accuracy is  93.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 9]  accuracy is  93.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 10]  accuracy is  93.66666666666667 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8]  was best, accuracy is  93.66666666666667 %\n",
      "\n",
      "\n",
      "On the  3 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 1]  accuracy is  86.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 2]  accuracy is  88.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 3]  accuracy is  88.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 4]  accuracy is  89.0 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5]  accuracy is  89.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 8, 9]  accuracy is  89.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 10]  accuracy is  88.0 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5]  was best, accuracy is  89.66666666666666 %\n",
      "\n",
      "\n",
      "On the  4 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 1]  accuracy is  84.33333333333334 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 2]  accuracy is  83.33333333333334 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3]  accuracy is  87.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 4]  accuracy is  87.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 9]  accuracy is  86.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 10]  accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3]  was best, accuracy is  87.33333333333333 %\n",
      "\n",
      "\n",
      "On the  5 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 1]  accuracy is  85.33333333333334 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 2]  accuracy is  81.66666666666667 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4]  accuracy is  86.0 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 9]  accuracy is  84.0 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 10]  accuracy is  83.33333333333334 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4]  was best, accuracy is  86.0 %\n",
      "\n",
      "\n",
      "On the  6 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 1]  accuracy is  78.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 2]  accuracy is  81.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 9]  accuracy is  80.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10]  accuracy is  82.66666666666667 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4, 10]  was best, accuracy is  82.66666666666667 %\n",
      "\n",
      "\n",
      "On the  7 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 1]  accuracy is  81.33333333333333 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 2]  accuracy is  81.0 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 9]  accuracy is  78.66666666666666 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4, 10, 1]  was best, accuracy is  81.33333333333333 %\n",
      "\n",
      "\n",
      "On the  8 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 1, 2]  accuracy is  79.66666666666666 %\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 1, 9]  accuracy is  78.33333333333333 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4, 10, 1, 2]  was best, accuracy is  79.66666666666666 %\n",
      "\n",
      "\n",
      "On the  9 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 5, 3, 4, 10, 1, 2, 9]  accuracy is  76.66666666666667 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6, 7, 8, 5, 3, 4, 10, 1, 2, 9]  was best, accuracy is  76.66666666666667 %\n",
      "\n",
      "\n",
      "Finished search!! The best feature subset is  [6, 7]  which has an accuracy of  94.0 %\n",
      "Function finishes in  14.977760314941406 seconds.\n"
     ]
    }
   ],
   "source": [
    "timed_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Xinning Dong's Feature Selection Algorithm.\n",
      "Type in the name of the file to test: CS205_small_testdata__25.txt\n",
      "Type the number of the algorithm you want to run.\n",
      "1) Forward Selection\n",
      "2) Backward Elimination\n",
      "2\n",
      "This data set has  10  features (not including the class attribute), with  300  instances.\n",
      "Running nearest neighbor with all  10  features, using \"leaving-one-out\" evaluation, I get an accuracy of  76.66666666666667 %\n",
      "Beginning search.\n",
      "\n",
      "\n",
      "On the  0 th level of the search tree\n",
      "\tUsing feature(s)  [2, 3, 4, 5, 6, 7, 8, 9, 10]  accuracy is  77.66666666666666 %\n",
      "\tUsing feature(s)  [1, 3, 4, 5, 6, 7, 8, 9, 10]  accuracy is  78.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 4, 5, 6, 7, 8, 9, 10]  accuracy is  78.0 %\n",
      "\tUsing feature(s)  [1, 2, 3, 5, 6, 7, 8, 9, 10]  accuracy is  77.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 7, 8, 9, 10]  accuracy is  80.0 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 7, 8, 9, 10]  accuracy is  73.66666666666667 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 6, 8, 9, 10]  accuracy is  71.66666666666667 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 6, 7, 9, 10]  accuracy is  76.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 6, 7, 8, 10]  accuracy is  79.66666666666666 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 5, 6, 7, 8, 9]  accuracy is  76.66666666666667 %\n",
      "\n",
      "\n",
      "Feature set  [1, 2, 3, 4, 6, 7, 8, 9, 10]  was best, accuracy is  80.0 %\n",
      "\n",
      "\n",
      "On the  1 th level of the search tree\n",
      "\tUsing feature(s)  [2, 3, 4, 6, 7, 8, 9, 10]  accuracy is  78.66666666666666 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 8, 9, 10]  accuracy is  82.0 %\n",
      "\tUsing feature(s)  [1, 2, 4, 6, 7, 8, 9, 10]  accuracy is  77.66666666666666 %\n",
      "\tUsing feature(s)  [1, 2, 3, 6, 7, 8, 9, 10]  accuracy is  78.0 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 7, 8, 9, 10]  accuracy is  74.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 8, 9, 10]  accuracy is  75.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 7, 9, 10]  accuracy is  77.33333333333333 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 7, 8, 10]  accuracy is  81.0 %\n",
      "\tUsing feature(s)  [1, 2, 3, 4, 6, 7, 8, 9]  accuracy is  78.0 %\n",
      "\n",
      "\n",
      "Feature set  [1, 3, 4, 6, 7, 8, 9, 10]  was best, accuracy is  82.0 %\n",
      "\n",
      "\n",
      "On the  2 th level of the search tree\n",
      "\tUsing feature(s)  [3, 4, 6, 7, 8, 9, 10]  accuracy is  82.66666666666667 %\n",
      "\tUsing feature(s)  [1, 4, 6, 7, 8, 9, 10]  accuracy is  80.66666666666666 %\n",
      "\tUsing feature(s)  [1, 3, 6, 7, 8, 9, 10]  accuracy is  83.0 %\n",
      "\tUsing feature(s)  [1, 3, 4, 7, 8, 9, 10]  accuracy is  74.33333333333333 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 8, 9, 10]  accuracy is  76.66666666666667 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 9, 10]  accuracy is  82.33333333333334 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 8, 10]  accuracy is  84.0 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 8, 9]  accuracy is  80.0 %\n",
      "\n",
      "\n",
      "Feature set  [1, 3, 4, 6, 7, 8, 10]  was best, accuracy is  84.0 %\n",
      "\n",
      "\n",
      "On the  3 th level of the search tree\n",
      "\tUsing feature(s)  [3, 4, 6, 7, 8, 10]  accuracy is  85.33333333333334 %\n",
      "\tUsing feature(s)  [1, 4, 6, 7, 8, 10]  accuracy is  86.33333333333333 %\n",
      "\tUsing feature(s)  [1, 3, 6, 7, 8, 10]  accuracy is  82.66666666666667 %\n",
      "\tUsing feature(s)  [1, 3, 4, 7, 8, 10]  accuracy is  74.0 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 8, 10]  accuracy is  78.33333333333333 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 10]  accuracy is  85.66666666666667 %\n",
      "\tUsing feature(s)  [1, 3, 4, 6, 7, 8]  accuracy is  81.0 %\n",
      "\n",
      "\n",
      "Feature set  [1, 4, 6, 7, 8, 10]  was best, accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "On the  4 th level of the search tree\n",
      "\tUsing feature(s)  [4, 6, 7, 8, 10]  accuracy is  87.0 %\n",
      "\tUsing feature(s)  [1, 6, 7, 8, 10]  accuracy is  86.0 %\n",
      "\tUsing feature(s)  [1, 4, 7, 8, 10]  accuracy is  71.66666666666667 %\n",
      "\tUsing feature(s)  [1, 4, 6, 8, 10]  accuracy is  77.33333333333333 %\n",
      "\tUsing feature(s)  [1, 4, 6, 7, 10]  accuracy is  85.66666666666667 %\n",
      "\tUsing feature(s)  [1, 4, 6, 7, 8]  accuracy is  84.66666666666667 %\n",
      "\n",
      "\n",
      "Feature set  [4, 6, 7, 8, 10]  was best, accuracy is  87.0 %\n",
      "\n",
      "\n",
      "On the  5 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 8, 10]  accuracy is  88.0 %\n",
      "\tUsing feature(s)  [4, 7, 8, 10]  accuracy is  72.66666666666667 %\n",
      "\tUsing feature(s)  [4, 6, 8, 10]  accuracy is  81.66666666666667 %\n",
      "\tUsing feature(s)  [4, 6, 7, 10]  accuracy is  89.0 %\n",
      "\tUsing feature(s)  [4, 6, 7, 8]  accuracy is  89.0 %\n",
      "\n",
      "\n",
      "Feature set  [4, 6, 7, 10]  was best, accuracy is  89.0 %\n",
      "\n",
      "\n",
      "On the  6 th level of the search tree\n",
      "\tUsing feature(s)  [6, 7, 10]  accuracy is  93.66666666666667 %\n",
      "\tUsing feature(s)  [4, 7, 10]  accuracy is  71.33333333333334 %\n",
      "\tUsing feature(s)  [4, 6, 10]  accuracy is  82.33333333333334 %\n",
      "\tUsing feature(s)  [4, 6, 7]  accuracy is  91.66666666666666 %\n",
      "\n",
      "\n",
      "Feature set  [6, 7, 10]  was best, accuracy is  93.66666666666667 %\n",
      "\n",
      "\n",
      "On the  7 th level of the search tree\n",
      "\tUsing feature(s)  [7, 10]  accuracy is  73.0 %\n",
      "\tUsing feature(s)  [6, 10]  accuracy is  85.0 %\n",
      "\tUsing feature(s)  [6, 7]  accuracy is  94.0 %\n",
      "\n",
      "\n",
      "Feature set  [6, 7]  was best, accuracy is  94.0 %\n",
      "\n",
      "\n",
      "On the  8 th level of the search tree\n",
      "\tUsing feature(s)  [7]  accuracy is  71.66666666666667 %\n",
      "\tUsing feature(s)  [6]  accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  [6]  was best, accuracy is  86.33333333333333 %\n",
      "\n",
      "\n",
      "On the  9 th level of the search tree\n",
      "\tUsing feature(s)  []  accuracy is  81.66666666666667 %\n",
      "\n",
      "\n",
      "(Warning, Accuracy has decreases! Continuing search in case of local maxima)\n",
      "Feature set  []  was best, accuracy is  81.66666666666667 %\n",
      "\n",
      "\n",
      "Finished search!! The best feature subset is  [6, 7]  which has an accuracy of  94.0 %\n",
      "Function finishes in  26.38103437423706 seconds.\n"
     ]
    }
   ],
   "source": [
    "timed_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
